Deep neural networks (DNNs) have been widely used in many machine learning tasks and achieve extraordinary performance. However, many factors can result in the network being vulnerable to adversarial samples. Recent studies have revealed that the output of DNNs can be easily modified by introducing relatively small perturbations to the input vector. In this study, we introduce a modified version of the Jacobian-based Saliency Map Attack (JSMA) method that achieves greater imperceptibility by altering pixels at areas of high-variance and adding additional $L_2$-regularization to the perturbed images. We demonstrate the enhanced imperceptibility of adversarial examples on the CIFAR-10 dataset. The result shows that our proposed method effectively increases the imperceptibility of the adversarial images compared to the baseline JSMA at the cost of a lower attack success rate.

The figure illustrates imperceptibility improved from our proposed algorithm compared to the original JSMA. The first column shows the image attacked by JSMA, and the second column shows the image attacked by JSMA-VW (which is the proposed algorithm with variance weighting and regularization).

Attacks against neural networks using adversarial examples have emerged as one of the most critical security challenges in the field of artificial intelligence. Traditional adversarial attacks do not consider both robustness and imperceptibility. In this study, we investigate the issue of adversarial attacks on image analysis using deep learning and proposed JSMA-VW, a method designed to target pixel modifications in regions of high-variance and incorporate extra $L_2$-regularization to the perturbed image, to improve the perceptual quality of adversarial examples while maintaining the fooling rate. Our experimental results show that by targeting pixel modifications in high-variance regions and adding minimization, JSMA-VW significantly improves the imperceptibility of adversarial examples to human observers compared to traditional JSMA methods, while sacrificing its ability to deceive image classification neural networks.
