# Adversarial-Attack

Deep neural networks (DNNs) have been widely used in many machine learning tasks and achieve extraordinary performance. However, many factors can result in the network being vulnerable to adversarial samples. Recent studies have revealed that the output of DNNs can be easily modified by introducing relatively small perturbations to the input vector. In this study, we introduce a modified version of the Jacobian-based Saliency Map Attack (JSMA) method that achieves greater imperceptibility by altering pixels at areas of high-variance and adding additional $L_2$-regularization to the perturbed images. We demonstrate the enhanced imperceptibility of adversarial examples on the CIFAR-10 dataset. The result shows that our proposed method effectively increases the imperceptibility of the adversarial images compared to the baseline JSMA at the cost of a lower attack success rate.

The figure illustrates imperceptibility improved from our proposed algorithm compared to the original JSMA. The first column shows the image attacked by JSMA, and the second column shows the image attacked by JSMA-VW (which is the proposed algorithm with variance weighting and regularization).

